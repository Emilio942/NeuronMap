"""Advanced experimental techniques for neural network analysis.

This module implements cutting-edge analysis methods including:
- Causality analysis between neurons
- Adversarial example generation and robustness testing
- Counterfactual analysis with modified inputs
- Mechanistic interpretability with circuit analysis
- Feature visualization using activation maximization techniques
"""

import logging
import numpy as np
import torch
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Union, Any
from pathlib import Path
import json
from dataclasses import dataclass
from scipy.stats import pearsonr, spearmanr
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mutual_info_score
import matplotlib.pyplot as plt
import seaborn as sns

logger = logging.getLogger(__name__)


@dataclass
class CausalityResult:
    """Results from causality analysis."""
    source_neuron: str
    target_neuron: str
    causality_score: float
    p_value: float
    method: str
    confidence_interval: Tuple[float, float]


@dataclass
class AdversarialResult:
    """Results from adversarial analysis."""
    original_text: str
    adversarial_text: str
    original_prediction: Any
    adversarial_prediction: Any
    perturbation_distance: float
    success: bool
    attack_method: str


@dataclass
class CounterfactualResult:
    """Results from counterfactual analysis."""
    original_text: str
    counterfactual_text: str
    changed_concepts: List[str]
    activation_changes: Dict[str, np.ndarray]
    effect_magnitude: float


class CausalityAnalyzer:
    """Analyze causal relationships between neurons."""

    def __init__(self, config_name: str = "default"):
        """Initialize causality analyzer."""
        from src.utils.config_manager import get_config
        self.config = get_config().get_experiment_config(config_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def granger_causality(self, x: np.ndarray, y: np.ndarray,
                         max_lag: int = 5) -> CausalityResult:
        """
        Compute Granger causality between two neuron activation time series.

        Args:
            x: Source neuron activations (time series)
            y: Target neuron activations (time series)
            max_lag: Maximum lag to consider

        Returns:
            CausalityResult with causality strength and significance
        """
        try:
            from statsmodels.tsa.stattools import grangercausalitytests

            # Prepare data for Granger causality test
            data = np.column_stack((y, x))  # Target, then source

            # Run Granger causality test
            results = grangercausalitytests(data, max_lag, verbose=False)

            # Extract best lag result
            best_lag = 1
            best_pvalue = float('inf')
            best_fstat = 0.0

            for lag in range(1, max_lag + 1):
                pvalue = results[lag][0]['ssr_ftest'][1]
                fstat = results[lag][0]['ssr_ftest'][0]

                if pvalue < best_pvalue:
                    best_pvalue = pvalue
                    best_fstat = fstat
                    best_lag = lag

            # Calculate causality score (normalized F-statistic)
            causality_score = best_fstat / (1 + best_fstat)

            # Estimate confidence interval (simplified)
            ci_lower = max(0, causality_score - 0.1)
            ci_upper = min(1, causality_score + 0.1)

            return CausalityResult(
                source_neuron="source",
                target_neuron="target",
                causality_score=causality_score,
                p_value=best_pvalue,
                method="granger",
                confidence_interval=(ci_lower, ci_upper)
            )

        except Exception as e:
            logger.error(f"Granger causality analysis failed: {e}")
            return CausalityResult(
                source_neuron="source",
                target_neuron="target",
                causality_score=0.0,
                p_value=1.0,
                method="granger",
                confidence_interval=(0.0, 0.0)
            )

    def transfer_entropy(self, x: np.ndarray, y: np.ndarray) -> CausalityResult:
        """
        Compute transfer entropy between two neuron activation series.

        Args:
            x: Source neuron activations
            y: Target neuron activations

        Returns:
            CausalityResult with transfer entropy score
        """
        try:
            # Discretize activations for mutual information calculation
            x_discrete = np.digitize(x, np.percentile(x, [25, 50, 75]))
            y_discrete = np.digitize(y, np.percentile(y, [25, 50, 75]))

            # Compute lagged versions
            y_past = y_discrete[:-1]
            y_present = y_discrete[1:]
            x_past = x_discrete[:-1]

            # Calculate mutual information components
            mi_y_present_y_past = mutual_info_score(y_present, y_past)

            # Create joint distribution for conditional MI
            joint_past = y_past * 10 + x_past  # Simple joint encoding
            mi_y_present_joint_past = mutual_info_score(y_present, joint_past)

            # Transfer entropy = MI(Y_t; X_{t-1} | Y_{t-1})
            transfer_entropy = mi_y_present_joint_past - mi_y_present_y_past

            # Normalize to [0, 1]
            te_normalized = max(0, transfer_entropy) / (np.log(4) + 1e-8)  # 4 possible states

            return CausalityResult(
                source_neuron="source",
                target_neuron="target",
                causality_score=te_normalized,
                p_value=0.05,  # Simplified
                method="transfer_entropy",
                confidence_interval=(max(0, te_normalized - 0.1),
                                   min(1, te_normalized + 0.1))
            )

        except Exception as e:
            logger.error(f"Transfer entropy analysis failed: {e}")
            return CausalityResult(
                source_neuron="source",
                target_neuron="target",
                causality_score=0.0,
                p_value=1.0,
                method="transfer_entropy",
                confidence_interval=(0.0, 0.0)
            )


class AdversarialAnalyzer:
    """Generate and analyze adversarial examples."""

    def __init__(self, config_name: str = "default"):
        """Initialize adversarial analyzer."""
        from src.utils.config_manager import get_config
        self.config = get_config().get_experiment_config(config_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def gradient_based_attack(self, model, tokenizer, text: str,
                            target_layer: str, epsilon: float = 0.1) -> AdversarialResult:
        """
        Generate adversarial examples using gradient-based attacks.

        Args:
            model: Language model
            tokenizer: Model tokenizer
            text: Input text
            target_layer: Layer to target for adversarial perturbation
            epsilon: Perturbation magnitude

        Returns:
            AdversarialResult with original and adversarial texts
        """
        try:
            # Tokenize input
            inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
            input_ids = inputs["input_ids"].to(self.device)

            # Get original embeddings
            embeddings = model.get_input_embeddings()(input_ids)
            embeddings.requires_grad_(True)

            # Forward pass to get activations
            with torch.enable_grad():
                outputs = model(inputs_embeds=embeddings)

                # Extract target layer activation (simplified)
                if hasattr(outputs, 'hidden_states') and outputs.hidden_states:
                    target_activation = outputs.hidden_states[-1].mean(dim=1)
                else:
                    target_activation = outputs.logits.mean(dim=1)

                # Create adversarial objective (maximize activation change)
                loss = -target_activation.sum()

            # Compute gradients
            loss.backward()
            gradient = embeddings.grad

            # Generate adversarial embeddings
            adversarial_embeddings = embeddings + epsilon * gradient.sign()

            # Generate adversarial text (simplified - use nearest tokens)
            adversarial_tokens = []
            vocab_embeddings = model.get_input_embeddings().weight

            for i in range(adversarial_embeddings.size(1)):
                adv_embed = adversarial_embeddings[0, i]
                # Find nearest token
                distances = torch.norm(vocab_embeddings - adv_embed, dim=1)
                nearest_token = torch.argmin(distances).item()
                adversarial_tokens.append(nearest_token)

            adversarial_text = tokenizer.decode(adversarial_tokens, skip_special_tokens=True)

            # Calculate perturbation distance
            perturbation_distance = torch.norm(gradient).item()

            return AdversarialResult(
                original_text=text,
                adversarial_text=adversarial_text,
                original_prediction=None,  # Would need specific task
                adversarial_prediction=None,
                perturbation_distance=perturbation_distance,
                success=True,
                attack_method="gradient_based"
            )

        except Exception as e:
            logger.error(f"Gradient-based attack failed: {e}")
            return AdversarialResult(
                original_text=text,
                adversarial_text=text,
                original_prediction=None,
                adversarial_prediction=None,
                perturbation_distance=0.0,
                success=False,
                attack_method="gradient_based"
            )

    def token_substitution_attack(self, model, tokenizer, text: str,
                                num_substitutions: int = 3) -> AdversarialResult:
        """
        Generate adversarial examples using token substitution.

        Args:
            model: Language model
            tokenizer: Model tokenizer
            text: Input text
            num_substitutions: Number of tokens to substitute

        Returns:
            AdversarialResult with substituted text
        """
        try:
            words = text.split()
            if len(words) < num_substitutions:
                num_substitutions = len(words)

            # Simple synonym substitution (in practice, use proper NLP library)
            synonym_dict = {
                'good': 'bad', 'bad': 'good', 'great': 'terrible', 'terrible': 'great',
                'love': 'hate', 'hate': 'love', 'excellent': 'awful', 'awful': 'excellent',
                'amazing': 'horrible', 'horrible': 'amazing', 'wonderful': 'terrible'
            }

            adversarial_words = words.copy()
            substitutions = 0

            for i, word in enumerate(words):
                if substitutions >= num_substitutions:
                    break

                word_lower = word.lower().strip('.,!?')
                if word_lower in synonym_dict:
                    adversarial_words[i] = synonym_dict[word_lower]
                    substitutions += 1

            adversarial_text = ' '.join(adversarial_words)

            # Calculate simple distance
            perturbation_distance = substitutions / len(words)

            return AdversarialResult(
                original_text=text,
                adversarial_text=adversarial_text,
                original_prediction=None,
                adversarial_prediction=None,
                perturbation_distance=perturbation_distance,
                success=adversarial_text != text,
                attack_method="token_substitution"
            )

        except Exception as e:
            logger.error(f"Token substitution attack failed: {e}")
            return AdversarialResult(
                original_text=text,
                adversarial_text=text,
                original_prediction=None,
                adversarial_prediction=None,
                perturbation_distance=0.0,
                success=False,
                attack_method="token_substitution"
            )


class CounterfactualAnalyzer:
    """Analyze counterfactual scenarios and their effects on activations."""

    def __init__(self, config_name: str = "default"):
        """Initialize counterfactual analyzer."""
        from src.utils.config_manager import get_config
        self.config = get_config().get_experiment_config(config_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def generate_counterfactuals(self, original_text: str,
                               concepts_to_change: List[str]) -> List[CounterfactualResult]:
        """
        Generate counterfactual texts by modifying specific concepts.

        Args:
            original_text: Original input text
            concepts_to_change: List of concepts to modify

        Returns:
            List of CounterfactualResult objects
        """
        results = []

        try:
            # Simple counterfactual generation (in practice, use more sophisticated methods)
            concept_mappings = {
                'sentiment': {
                    'positive': ['good', 'great', 'excellent', 'amazing', 'wonderful'],
                    'negative': ['bad', 'terrible', 'awful', 'horrible', 'worst'],
                    'neutral': ['okay', 'standard', 'normal', 'regular', 'typical']
                },
                'tense': {
                    'past': ['was', 'had', 'did', 'went', 'came'],
                    'present': ['is', 'has', 'does', 'goes', 'comes'],
                    'future': ['will be', 'will have', 'will do', 'will go', 'will come']
                },
                'negation': {
                    'positive': ['is', 'can', 'will', 'does', 'has'],
                    'negative': ['is not', 'cannot', 'will not', 'does not', 'has not']
                }
            }

            for concept in concepts_to_change:
                if concept in concept_mappings:
                    counterfactual_text = self._apply_concept_change(
                        original_text, concept, concept_mappings[concept]
                    )

                    result = CounterfactualResult(
                        original_text=original_text,
                        counterfactual_text=counterfactual_text,
                        changed_concepts=[concept],
                        activation_changes={},  # Would be computed with model
                        effect_magnitude=self._calculate_text_similarity(
                            original_text, counterfactual_text
                        )
                    )
                    results.append(result)

            return results

        except Exception as e:
            logger.error(f"Counterfactual generation failed: {e}")
            return []

    def _apply_concept_change(self, text: str, concept: str,
                            mappings: Dict[str, List[str]]) -> str:
        """Apply concept changes to text."""
        words = text.split()
        modified_words = words.copy()

        # Simple word replacement based on concept
        if concept == 'sentiment':
            # Find sentiment words and flip them
            for i, word in enumerate(words):
                word_lower = word.lower().strip('.,!?')
                for sentiment_type, sentiment_words in mappings.items():
                    if word_lower in sentiment_words:
                        # Flip to opposite sentiment
                        if sentiment_type == 'positive':
                            modified_words[i] = np.random.choice(mappings['negative'])
                        elif sentiment_type == 'negative':
                            modified_words[i] = np.random.choice(mappings['positive'])
                        break

        return ' '.join(modified_words)

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """Calculate similarity between two texts."""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1 and not words2:
            return 1.0

        intersection = words1 & words2
        union = words1 | words2

        return len(intersection) / len(union) if union else 0.0


class MechanisticInterpreter:
    """Analyze mechanistic interpretability through circuit analysis."""

    def __init__(self, config_name: str = "default"):
        """Initialize mechanistic interpreter."""
        from src.utils.config_manager import get_config
        self.config = get_config().get_experiment_config(config_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def analyze_attention_circuits(self, attention_weights: np.ndarray,
                                 tokens: List[str]) -> Dict[str, Any]:
        """
        Analyze attention patterns to identify computational circuits.

        Args:
            attention_weights: Attention weights [layers, heads, seq_len, seq_len]
            tokens: List of tokens

        Returns:
            Dictionary with circuit analysis results
        """
        try:
            results = {
                'circuit_patterns': {},
                'head_specializations': {},
                'information_flow': {},
                'critical_connections': []
            }

            n_layers, n_heads, seq_len, _ = attention_weights.shape

            # Analyze head specializations
            for layer in range(n_layers):
                for head in range(n_heads):
                    head_weights = attention_weights[layer, head]

                    # Calculate attention patterns
                    diagonal_attention = np.mean(np.diag(head_weights))
                    local_attention = np.mean([head_weights[i, max(0, i-2):i+3].sum()
                                             for i in range(seq_len)])

                    head_key = f"layer_{layer}_head_{head}"
                    results['head_specializations'][head_key] = {
                        'diagonal_attention': diagonal_attention,
                        'local_attention': local_attention,
                        'pattern_type': self._classify_attention_pattern(head_weights)
                    }

            # Analyze information flow between layers
            for layer in range(n_layers - 1):
                current_layer_attention = attention_weights[layer].mean(axis=0)
                next_layer_attention = attention_weights[layer + 1].mean(axis=0)

                # Calculate flow correlation
                flow_correlation = np.corrcoef(
                    current_layer_attention.flatten(),
                    next_layer_attention.flatten()
                )[0, 1]

                results['information_flow'][f"layer_{layer}_to_{layer+1}"] = flow_correlation

            # Identify critical connections
            all_weights = attention_weights.reshape(-1, seq_len, seq_len)
            mean_weights = np.mean(all_weights, axis=0)

            # Find strongest attention connections
            threshold = np.percentile(mean_weights, 95)
            critical_positions = np.where(mean_weights > threshold)

            for i, (from_pos, to_pos) in enumerate(zip(critical_positions[0], critical_positions[1])):
                if from_pos < len(tokens) and to_pos < len(tokens):
                    results['critical_connections'].append({
                        'from_token': tokens[from_pos],
                        'to_token': tokens[to_pos],
                        'weight': mean_weights[from_pos, to_pos],
                        'position_from': int(from_pos),
                        'position_to': int(to_pos)
                    })

            return results

        except Exception as e:
            logger.error(f"Attention circuit analysis failed: {e}")
            return {}

    def _classify_attention_pattern(self, attention_matrix: np.ndarray) -> str:
        """Classify the type of attention pattern."""
        try:
            seq_len = attention_matrix.shape[0]

            # Calculate pattern metrics
            diagonal_strength = np.mean(np.diag(attention_matrix))

            # Local attention (within 3 positions)
            local_mask = np.abs(np.arange(seq_len)[:, None] - np.arange(seq_len)) <= 2
            local_strength = np.mean(attention_matrix[local_mask])

            # Global attention
            global_strength = np.mean(attention_matrix)

            # Classify pattern
            if diagonal_strength > 0.5:
                return "self_attention"
            elif local_strength > global_strength * 1.5:
                return "local_attention"
            elif np.max(attention_matrix) > 0.8:
                return "focused_attention"
            else:
                return "distributed_attention"

        except Exception:
            return "unknown"


class AdvancedExperimentalPipeline:
    """Pipeline for running all advanced experimental analyses."""

    def __init__(self, config_name: str = "default"):
        """Initialize advanced experimental pipeline."""
        self.config_name = config_name
        self.causality_analyzer = CausalityAnalyzer(config_name)
        self.adversarial_analyzer = AdversarialAnalyzer(config_name)
        self.counterfactual_analyzer = CounterfactualAnalyzer(config_name)
        self.mechanistic_interpreter = MechanisticInterpreter(config_name)

    def run_full_analysis(self, model, tokenizer, texts: List[str],
                         output_dir: str) -> Dict[str, Any]:
        """
        Run comprehensive advanced experimental analysis.

        Args:
            model: Language model
            tokenizer: Model tokenizer
            texts: List of input texts
            output_dir: Directory to save results

        Returns:
            Dictionary with all analysis results
        """
        logger.info("Running advanced experimental analysis...")

        results = {
            'causality_analysis': {},
            'adversarial_analysis': [],
            'counterfactual_analysis': [],
            'mechanistic_analysis': {},
            'metadata': {
                'n_texts': len(texts),
                'config': self.config_name,
                'timestamp': Path(output_dir).name
            }
        }

        # Create output directory
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        try:
            # Run adversarial analysis
            logger.info("Running adversarial analysis...")
            for text in texts[:5]:  # Limit for efficiency
                adv_result = self.adversarial_analyzer.gradient_based_attack(
                    model, tokenizer, text
                )
                results['adversarial_analysis'].append(adv_result.__dict__)

            # Run counterfactual analysis
            logger.info("Running counterfactual analysis...")
            for text in texts[:5]:  # Limit for efficiency
                cf_results = self.counterfactual_analyzer.generate_counterfactuals(
                    text, ['sentiment', 'negation']
                )
                for cf_result in cf_results:
                    results['counterfactual_analysis'].append(cf_result.__dict__)

            # Save results
            with open(output_path / "advanced_experimental_results.json", 'w') as f:
                json.dump(results, f, indent=2, default=str)

            # Create visualizations
            self._create_visualizations(results, output_path)

            logger.info(f"Advanced experimental analysis completed. Results saved to {output_dir}")
            return results

        except Exception as e:
            logger.error(f"Advanced experimental analysis failed: {e}")
            return results

    def _create_visualizations(self, results: Dict[str, Any], output_path: Path):
        """Create visualizations for analysis results."""
        try:
            # Adversarial analysis visualization
            if results['adversarial_analysis']:
                perturbation_distances = [
                    r['perturbation_distance'] for r in results['adversarial_analysis']
                ]

                plt.figure(figsize=(10, 6))
                plt.hist(perturbation_distances, bins=20, alpha=0.7)
                plt.xlabel('Perturbation Distance')
                plt.ylabel('Frequency')
                plt.title('Distribution of Adversarial Perturbation Distances')
                plt.savefig(output_path / "adversarial_perturbations.png")
                plt.close()

            # Counterfactual analysis visualization
            if results['counterfactual_analysis']:
                effect_magnitudes = [
                    r['effect_magnitude'] for r in results['counterfactual_analysis']
                ]

                plt.figure(figsize=(10, 6))
                plt.hist(effect_magnitudes, bins=20, alpha=0.7)
                plt.xlabel('Effect Magnitude')
                plt.ylabel('Frequency')
                plt.title('Distribution of Counterfactual Effect Magnitudes')
                plt.savefig(output_path / "counterfactual_effects.png")
                plt.close()

            logger.info("Visualizations created successfully")

        except Exception as e:
            logger.error(f"Visualization creation failed: {e}")


# Export main classes
__all__ = [
    'CausalityAnalyzer',
    'AdversarialAnalyzer',
    'CounterfactualAnalyzer',
    'MechanisticInterpreter',
    'AdvancedExperimentalPipeline',
    'CausalityResult',
    'AdversarialResult',
    'CounterfactualResult'
]
