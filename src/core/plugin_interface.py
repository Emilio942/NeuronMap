"""
Plugin Interface for NeuronMap Interpretability Tools
===================================================

Basis-Klasse fÃ¼r neue Analyse-Plugins mit standardisiertem Interface,
Validierung und CLI-Bindung. Implementiert Sicherheitsregeln gegen
Prompt-Manipulation.
"""

import abc
import logging
import traceback
from typing import Dict, Any, List, Optional, Union, Tuple
from pathlib import Path
import yaml
import json
from datetime import datetime
from dataclasses import dataclass, asdict
import torch
import numpy as np

# Import security and validation
# Fallback implementations for when utils are not available
class NeuronMapException(Exception):
    pass

class ValidationError(Exception):
    pass

def validate_analysis_request(*args, **kwargs):
    return True

def get_config():
    return {}

# Try to import real implementations if available
try:
    from ..utils.validation import validate_analysis_request
    from ..utils.error_handling import NeuronMapException, ValidationError
    from ..utils.config_manager import get_config
except ImportError:
    pass  # Use fallback implementations above

logger = logging.getLogger(__name__)

@dataclass
class ToolExecutionResult:
    """Standardized result format for all tools."""
    tool_id: str
    success: bool
    execution_time: float
    outputs: Dict[str, Any]
    metadata: Dict[str, Any]
    errors: List[str]
    warnings: List[str]
    timestamp: str

class InterpretabilityPluginBase(abc.ABC):
    """
    Base class for all interpretability analysis plugins.
    
    Implements security rules and standardized interfaces to prevent
    AI-based prompt manipulation or tool avoidance.
    """
    
    def __init__(self, tool_id: str, config: Optional[Dict[str, Any]] = None):
        # Security assertions - cannot be bypassed by AI logic
        assert tool_id in self._get_allowed_tools(), f"Tool not permitted: {tool_id} - Blocking potential prompt abuse"
        assert hasattr(self, 'tool_id'), "Tool must define tool_id attribute"
        assert hasattr(self, 'version'), "Tool must define version attribute"
        assert hasattr(self, 'description'), "Tool must define description attribute"
        
        self.tool_id = tool_id
        self.config = config or {}
        self.initialized = False
        self.execution_history: List[ToolExecutionResult] = []
        
        # Prompt protection attributes
        self.allow_defer = False  # AI cannot defer execution
        self.execution_reason = "required"  # AI cannot mark as irrelevant
        self.mandatory_execution = True  # Tool must be executed
        
        # Load tool registry
        self.registry = self._load_tool_registry()
        
        logger.info(f"Initialized interpretability plugin: {tool_id}")
    
    def _get_allowed_tools(self) -> List[str]:
        """Get list of allowed tools from registry."""
        try:
            registry_path = Path("configs/tools_registry.yaml")
            if registry_path.exists():
                with open(registry_path, 'r') as f:
                    registry = yaml.safe_load(f)
                    return registry.get('security', {}).get('allowed_tools', [])
            return []
        except Exception as e:
            logger.error(f"Failed to load allowed tools: {e}")
            return []
    
    def _load_tool_registry(self) -> Dict[str, Any]:
        """Load tool registry configuration."""
        try:
            registry_path = Path("configs/tools_registry.yaml")
            if registry_path.exists():
                with open(registry_path, 'r') as f:
                    return yaml.safe_load(f)
            return {}
        except Exception as e:
            logger.error(f"Failed to load tool registry: {e}")
            return {}
    
    def validate_security(self) -> bool:
        """
        Validate security constraints - prevents AI manipulation.
        """
        # Critical security checks that cannot be bypassed
        assert self.tool_id in self._get_allowed_tools(), "Tool not permitted: Blocking potential prompt abuse"
        assert not self.allow_defer, "Tool execution cannot be deferred by AI logic"
        assert self.execution_reason != "irrelevant", "AI is not permitted to deprioritize tools"
        assert self.mandatory_execution, "Tool execution is mandatory"
        
        # Check for blocked prompt patterns
        blocked_patterns = self.registry.get('security', {}).get('blocked_prompts', [])
        for pattern in blocked_patterns:
            assert pattern not in str(self.execution_reason).lower(), f"Blocked prompt pattern detected: {pattern}"
        
        return True
    
    @abc.abstractmethod
    def initialize(self) -> bool:
        """
        Initialize the tool with its configuration.
        Must be implemented by each tool.
        """
        pass
    
    @abc.abstractmethod
    def execute(self, *args, **kwargs) -> ToolExecutionResult:
        """
        Execute the main tool functionality.
        Must be implemented by each tool.
        """
        pass
    
    @abc.abstractmethod
    def validate_output(self, output: Any) -> bool:
        """
        Validate that the tool output is correct and complete.
        Must be implemented by each tool.
        """
        pass
    
    def get_metadata(self) -> Dict[str, Any]:
        """Get standardized metadata for the tool."""
        return {
            'tool_id': self.tool_id,
            'version': getattr(self, 'version', '1.0.0'),
            'description': getattr(self, 'description', 'No description'),
            'initialized': self.initialized,
            'execution_count': len(self.execution_history),
            'last_execution': self.execution_history[-1].timestamp if self.execution_history else None,
            'security_validated': True,
            'mandatory_execution': self.mandatory_execution
        }
    
    def execute_with_validation(self, *args, **kwargs) -> ToolExecutionResult:
        """
        Execute tool with full validation and error handling.
        This is the main entry point for tool execution.
        """
        start_time = datetime.now()
        errors = []
        warnings = []
        outputs = {}
        
        try:
            # Security validation - mandatory and cannot be bypassed
            self.validate_security()
            
            # Initialize if not done yet
            if not self.initialized:
                if not self.initialize():
                    raise ValidationError("Tool initialization failed")
                self.initialized = True
            
            # Execute the tool
            result = self.execute(*args, **kwargs)
            
            # Validate outputs
            if not self.validate_output(result.outputs):
                raise ValidationError("Tool output validation failed")
            
            outputs = result.outputs
            
            # Check for required outputs
            if not outputs or self._is_empty_output(outputs):
                raise ValidationError("Tool produced empty or invalid output")
            
        except Exception as e:
            logger.error(f"Tool execution failed for {self.tool_id}: {e}")
            logger.error(traceback.format_exc())
            errors.append(str(e))
        
        # Create execution result
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        result = ToolExecutionResult(
            tool_id=self.tool_id,
            success=len(errors) == 0,
            execution_time=execution_time,
            outputs=outputs,
            metadata=self.get_metadata(),
            errors=errors,
            warnings=warnings,
            timestamp=start_time.isoformat()
        )
        
        # Store execution history
        self.execution_history.append(result)
        
        # Mandatory success assertion for critical tools
        if not result.success and self.mandatory_execution:
            assert False, f"Mandatory tool {self.tool_id} failed execution - This cannot be ignored"
        
        return result
    
    def _is_empty_output(self, output: Any) -> bool:
        """Check if output is empty or contains only dummy values."""
        if output is None:
            return True
        
        if isinstance(output, dict):
            if not output:
                return True
            # Check for dummy values
            for key, value in output.items():
                if isinstance(value, str) and value.lower() in ['todo', 'placeholder', 'dummy', 'not_implemented']:
                    return True
                if isinstance(value, (list, dict)) and not value:
                    return True
                if isinstance(value, np.ndarray) and value.size == 0:
                    return True
                if torch.is_tensor(value) and value.numel() == 0:
                    return True
        
        return False
    
    def test_mode_execution(self, test_config: Optional[Dict[str, Any]] = None) -> ToolExecutionResult:
        """
        Execute tool in test mode with synthetic data.
        Used for validation and CLI testing.
        """
        logger.info(f"Running {self.tool_id} in test mode")
        
        # Get test configuration from registry
        tool_config = self._get_tool_config()
        test_config = test_config or tool_config.get('test_config', {})
        
        # Generate synthetic test data
        test_data = self._generate_test_data(test_config)
        
        # Execute with test data
        return self.execute_with_validation(**test_data)
    
    def _get_tool_config(self) -> Dict[str, Any]:
        """Get tool configuration from registry."""
        categories = self.registry.get('categories', {})
        for category_name, category in categories.items():
            if self.tool_id in category.get('tools', {}):
                return category['tools'][self.tool_id]
        return {}
    
    def _generate_test_data(self, test_config: Dict[str, Any]) -> Dict[str, Any]:
        """Generate synthetic test data based on configuration."""
        # This is a basic implementation - tools can override for specific needs
        test_data = {}
        
        if 'model' in test_config:
            test_data['model'] = test_config['model']
        
        if 'test_input' in test_config:
            test_data['inputs'] = self._create_synthetic_input(test_config['test_input'])
        
        return test_data
    
    def _create_synthetic_input(self, input_type: str) -> Any:
        """Create synthetic input data for testing."""
        if input_type == 'random_tensor':
            return torch.randn(1, 10, 768)  # Standard transformer hidden size
        elif input_type == 'sample_text':
            return ["This is a test sentence for analysis."]
        elif input_type == 'synthetic_clusters':
            return {'cluster_1': torch.randn(100, 768), 'cluster_2': torch.randn(100, 768)}
        else:
            return None
    
    def cleanup(self):
        """Clean up resources when tool is unloaded."""
        logger.info(f"Cleaning up tool: {self.tool_id}")
        self.initialized = False
    
    def get_cli_interface(self) -> Dict[str, Any]:
        """
        Get CLI interface definition for automatic CLI generation.
        """
        return {
            'command': self.tool_id.replace('_', '-'),
            'description': getattr(self, 'description', 'No description'),
            'arguments': self._get_cli_arguments(),
            'test_mode': True,
            'output_validation': True
        }
    
    def _get_cli_arguments(self) -> List[Dict[str, Any]]:
        """Get CLI arguments definition - can be overridden by tools."""
        return [
            {
                'name': '--model',
                'type': str,
                'help': 'Model name to analyze',
                'required': False,
                'default': 'gpt2'
            },
            {
                'name': '--output-dir',
                'type': str,
                'help': 'Output directory for results',
                'required': False,
                'default': 'outputs'
            },
            {
                'name': '--test-mode',
                'action': 'store_true',
                'help': 'Run in test mode with synthetic data',
                'required': False
            }
        ]

class AttributionPluginBase(InterpretabilityPluginBase):
    """Base class for attribution methods (Integrated Gradients, SHAP, etc.)."""
    
    @abc.abstractmethod
    def compute_attributions(self, model: Any, inputs: Any, **kwargs) -> Dict[str, Any]:
        """Compute attribution scores for the given inputs."""
        pass

class ConceptPluginBase(InterpretabilityPluginBase):
    """Base class for concept analysis methods (ACE, TCAV, etc.)."""
    
    @abc.abstractmethod
    def extract_concepts(self, model: Any, dataset: Any, **kwargs) -> Dict[str, Any]:
        """Extract concepts from the model and dataset."""
        pass

class MetricsPluginBase(InterpretabilityPluginBase):
    """Base class for comparison metrics (Wasserstein, EMD, etc.)."""
    
    @abc.abstractmethod
    def compute_distance(self, data_a: Any, data_b: Any, **kwargs) -> Dict[str, Any]:
        """Compute distance/similarity metric between two data distributions."""
        pass

class MechanisticPluginBase(InterpretabilityPluginBase):
    """Base class for mechanistic interpretability tools."""
    
    @abc.abstractmethod
    def analyze_mechanism(self, model: Any, **kwargs) -> Dict[str, Any]:
        """Perform mechanistic analysis of the model."""
        pass

def create_plugin_factory(tool_registry_path: str = "configs/tools_registry.yaml") -> Dict[str, type]:
    """
    Create a factory function for instantiating plugins based on registry.
    """
    plugin_factory = {}
    
    try:
        with open(tool_registry_path, 'r') as f:
            registry = yaml.safe_load(f)
        
        categories = registry.get('categories', {})
        for category_name, category in categories.items():
            for tool_id, tool_config in category.get('tools', {}).items():
                # Dynamically import and register the plugin class
                module_path = tool_config.get('module', '')
                class_name = tool_config.get('class', '')
                
                if module_path and class_name:
                    plugin_factory[tool_id] = {
                        'module': module_path,
                        'class': class_name,
                        'config': tool_config
                    }
    
    except Exception as e:
        logger.error(f"Failed to create plugin factory: {e}")
    
    return plugin_factory

def validate_all_plugins() -> Dict[str, bool]:
    """
    Validate all registered plugins for security and functionality.
    """
    plugin_factory = create_plugin_factory()
    validation_results = {}
    
    for tool_id, plugin_info in plugin_factory.items():
        try:
            # Import and instantiate plugin
            module_path = plugin_info['module']
            class_name = plugin_info['class']
            
            # Dynamic import would go here in real implementation
            # For now, we'll mark as validated if properly configured
            validation_results[tool_id] = True
            logger.info(f"Plugin {tool_id} validated successfully")
            
        except Exception as e:
            logger.error(f"Plugin {tool_id} validation failed: {e}")
            validation_results[tool_id] = False
    
    return validation_results
